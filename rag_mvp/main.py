import streamlit as st
from dotenv import load_dotenv
import os
import getpass
import hashlib
import pickle
import atexit

from rag_mvp.components.sidebar import sidebar
from ui import (
    wrap_doc_in_html,
    is_query_valid,
    is_file_valid,
    display_file_read_error,
)
from rag_mvp.core.caching import bootstrap_caching
from rag_mvp.core.parsing import read_file
from rag_mvp.core.chunking import chunk_file
from rag_mvp.core.embedding import embed_files
from rag_mvp.core.qa import query_folder
from rag_mvp.core.utils import get_llm

load_dotenv()

EMBEDDING = "openai"
VECTOR_STORE = "faiss"
MODEL_LIST = ["mistral-large-latest"]

if "MISTRAL_API_KEY" not in os.environ:
    os.environ["MISTRAL_API_KEY"] = getpass.getpass("Enter your Mistral API key: ")

st.set_page_config(page_title="RAG_MVP", page_icon="üîç", layout="wide")
st.header("üîç–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ AimateDocs")

# Enable caching for expensive functions
bootstrap_caching()

sidebar()

uploaded_files = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ pdf, docx, –∏–ª–∏ txt —Ñ–∞–π–ª",
    type=["pdf", "docx", "txt"],
    help="–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è.",
    accept_multiple_files=True,
)

model: str = st.selectbox("–ú–æ–¥–µ–ª—å", options=MODEL_LIST)  # type: ignore

with st.expander("–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –æ–ø—Ü–∏–∏"):
    return_all_chunks = st.checkbox("–ü–æ–∫–∞–∑–≤–∞—Ç—å –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º")
    chunk_size_input = st.text_input(
        "–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –±–ª–æ–∫–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)",
        type="default",
        placeholder="–ù–∞–ø–∏—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞",
        help="–ó–Ω–∞—á–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 100 –¥–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤",
        value=1024,
    )
    chunk_overlap_input = st.text_input(
        "–ù–∞–ª–æ–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)",
        type="default",
        placeholder="–ù–∞–ø–∏—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –±–ª–æ–∫–æ–≤",
        help="–ó–Ω–∞—á–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ—Ç 0 –¥–æ 1500 —Å–∏–º–≤–æ–ª–æ–≤",
        value=400,
    )
    chunk_size_input = int(chunk_size_input)
    chunk_overlap_input = int(chunk_overlap_input)
    show_full_doc = False

if not any(uploaded_file for uploaded_file in uploaded_files):
    st.stop()

if not (100 <= chunk_size_input <= 2000 and 0 <= chunk_overlap_input <= 1500):
    st.stop()

def store_indexed_data(file_id, indexed_data):
    with open(f'indexed_data_cache_{file_id}.pkl', 'wb') as f:
        pickle.dump(indexed_data, f)

def get_indexed_data(file_id):
    try:
        with open(f'indexed_data_cache_{file_id}.pkl', 'rb') as f:
            return pickle.load(f)
    except FileNotFoundError:
        return None

files = []
for file in uploaded_files:
    try:
        files.append(read_file(file))
    except Exception as e:
        display_file_read_error(e, file_name=file.name)

chunked_files = []
for file in files:
    cached_data = get_indexed_data(file.id)

    if cached_data:
        chunked_files.append(cached_data)
    else:
        chunked_file = chunk_file(file, chunk_size=chunk_size_input, chunk_overlap=chunk_overlap_input)
        store_indexed_data(file.id, chunked_file)
        chunked_files.append(chunked_file)

if not any(is_file_valid(chunked_file) for chunked_file in chunked_files):
    st.stop()

def store_folder_index(file_id, folder_index):
    with open(f'folder_index_cache_{file_id}.pkl', 'wb') as f:
        pickle.dump(folder_index, f)

def get_folder_index(file_id):
    try:
        with open(f'folder_index_cache_{file_id}.pkl', 'rb') as f:
            return pickle.load(f)
    except FileNotFoundError:
        return None

# Compute a combined hash for the uploaded files
files_hash = hashlib.md5("".join(file.id for file in files).encode()).hexdigest()

# Check if the folder index is already cached
folder_index = get_folder_index(files_hash)

if not folder_index:
    with st.spinner("–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è‚è≥"):
        folder_index = embed_files(
            files=chunked_files,
            embedding=EMBEDDING if model != "debug" else "debug",
            vector_store=VECTOR_STORE if model != "debug" else "debug",
        )
        store_folder_index(files_hash, folder_index)

with st.form(key="qa_form"):
    query = st.text_area("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É")
    submit = st.form_submit_button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å")

if show_full_doc:
    with st.expander("Document"):
        st.markdown(f"<p>{wrap_doc_in_html(file.docs)}</p>", unsafe_allow_html=True)

if submit:
    if not is_query_valid(query):
        st.stop()

    answer_col, sources_col = st.columns(2)

    llm = get_llm(model=model, temperature=0)
    result = query_folder(
        folder_index=folder_index,
        query=query,
        return_all=return_all_chunks,
        llm=llm,
    )

    with answer_col:
        st.markdown("#### –û—Ç–≤–µ—Ç")
        st.markdown(result.answer)

    with sources_col:
        st.markdown("#### –ò—Å—Ç–æ—á–Ω–∏–∫–∏")
        for source in result.sources:
            st.markdown(source.page_content)
            st.markdown(source.metadata["source"])
            st.markdown("---")

def cleanup():
    for file in uploaded_files:
        try:
            os.remove(f'indexed_data_cache_{file.id}.pkl')
        except FileNotFoundError:
            pass
    try:
        os.remove(f'folder_index_cache_{files_hash}.pkl')
    except FileNotFoundError:
        pass

atexit.register(cleanup)